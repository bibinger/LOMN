#############################################################################################
# Function          Description of function (arguments explained in function)
#############################################################################################
# sq.vola.global    function computes the spot variance in the LOMN model 
# sim.global.one    wraps the steps for the simulation in the LOMN model (asymptotic critical values)
# boot.global       function conducts the bootstrap for critical values of the global test
# boot.global.round function conducts the bootstrap for critical values of the global test for the setup with rounding errors
# sim.global        wraps the steps for the simulation of the global test LOMN vs MMN (bootstrap for critical values)
# sim.global.round  wraps the steps for the simulation of the global test LOMN vs MMN (bootstrap for critical values, data generating process with rounding errors)
# boot.local        function conducts the bootstrap for critical values of the local test
# sim.local         wraps the steps for the simulation of the local test LOMN vs MMN (bootstrap for critical values)
#############################################################################################

sq.vola.global = function(Y, # noisy observations generated by LOMN-model
                          h, # number of observations per time interval (local minima)
                          K, # width of the smoothing window
                          n  # number of observations
                          ){
  
  # define data points to compute local minima
  sgrid = seq(h + 1, n, by = h); g = length(sgrid)
  datpoints = c(rep_row(sgrid - (K + 1) * h, 2 * K + 2) + rep_col(seq(0, (2 * K + 1) * h, by = h), length(sgrid)))
  datpoints[datpoints < h | datpoints > n - h] = NA
  
  
  # compute spot variance
  sq.vola = n/(2 * h) * pi/(pi - 2) * colMeans(matrix(c(NA, diff(row_min(t(matrix(Y[rep_row(datpoints, h) + rep_col(0:(h - 1), length(datpoints))], nrow = h))))^2), nrow = 2 * (K + 1))[-1, ], na.rm = T)
  
  
  # return results
  c(rep(sq.vola[1], ceiling(sum(sgrid[1:2])/2 - 1)),
    sapply(sq.vola[-c(1, g)], rep, h),
    rep(sq.vola[g], floor(n - sum(sgrid[g:(g - 1)])/2 + 1)))
}

#############################################################################################

sim.global.one = function(n, 
                          q){
  
  # volatility model
  sigma2 = numeric(n); sigma2[1] = .8465
  
      BM = sqrt(1/n) * matrix(zrnorm(n = 2 * n), 
                              ncol = 2) %*% 
                       matrix(c(-0.8660254, -0.5, 0.8660254, -0.5), 
                              ncol = 2)
  
  for(i in 1:(n - 1)){
    sigma2[i + 1] = sigma2[i] + 0.0162 * (0.8465 - sigma2[i])/n + 0.117 * sqrt(sigma2[i]) * BM[i, 2]
  }
  
  # rescale squared volatility
  sigma2 = sigma2 * ((6 - sin(3/4 * pi * (1:n)/n))/5 * 0.01)^2
  
  
  # equilibrium log-price process under H0
  X = c(filter(x = c(0, sqrt(sigma2) * BM[, 1]), 
               filter = 1, 
               method = "recursive", 
               init = 1))
  
  
  # define jump size, jump time and equilibrium jump-log-price process under H1
  jump.size = c(0.1, 0.2, 0.3, 0.4, 0.5) / 100
  d = length(jump.size)
  X.jump = matrix(0, 
                  nrow = length(X), 
                  ncol = 2 * d)
  jump.time = sample(2850:20580, 1)
  for(j in 1:d){
    X.jump[, j] = X
    X.jump[jump.time:(n + 1), j] = X.jump[jump.time:(n + 1), j] + jump.size[j]
  }
  for(j in 1:d){
    X.jump[, d + j] = X
    X.jump[jump.time:(n + 1), d + j] = X.jump[jump.time:(n + 1), d + j] - jump.size[j]
  }  
  
  
  # two-sided autocorrelated noise  
  eta = rnorm(n = n + 1, 
              sd = q)
  
  for(k in 1:n){
    eta[k + 1] = eta[k + 1] - 0.5 * eta[k]
  }
  
  
  # two processes: hypothesis (no jump) and alternative (with jump) for two-sided noise
  Y.two = cbind(1:(n + 1), X + eta)
  Y.two.jump = cbind(1:(n + 1), X.jump + matrix(rep(eta, 2 * d), ncol = 2 * d))
  
  # two processes: hypothesis (no jump) and alternative (with jump) for one-sided noise
  Y.one = Y.two[which(eta >= 0), ]
  Y.one.jump = Y.two.jump[which(eta >= 0), ]
  
  n = NROW(Y.one)
  
  # length of local windows
  h.one2zero = 1.3 * n^(-2/3) # as defined in theory
  h.one = ceiling(n * h.one2zero) # transformed to number of observations per window
  
  
  # spot variance estimation
  Sq.vola.one = sq.vola.global(Y = Y.one[, 2],
                               h = 30, 
                               K = 100,
                               n = n) * 0.954
  
  
  # define data points used for the local windows to compute local minima
  datpoints.one = rep_row(seq(1, n, by = h.one), h.one) + rep_col(0:(h.one - 1), ceiling(n/h.one))
  
  
  # test statistics under H0
  test.one = abs(diff(row_min(t(matrix(Y.one[, 2][datpoints.one], nrow = h.one)))))/sqrt(Sq.vola.one[round(colMeans(datpoints.one))][-1])
  test.one = max(test.one, na.rm = T)
  
  
  # test statistics under H1
  test.one.jump = test.one.time = numeric(2 * d)
  for(j in 1:(2 * d)){
    Xtest.one.jump   = abs(diff(row_min(t(matrix(Y.one.jump[, j + 1][datpoints.one], nrow = h.one)))))/sqrt(Sq.vola.one[round(colMeans(datpoints.one))][-1])
    test.one.time[j] = mean(matrix(Y.one.jump[, 1][datpoints.one], nrow = h.one)[, which.max(Xtest.one.jump)])
    test.one.jump[j] = max(Xtest.one.jump, na.rm = T)  
  }
  
  # quantile of Gumbel-distribution
  quant.asymp = -log(-log(0.95))
  
  
  # rescale test-statistics               
        .l = log(2 * (1 / h.one2zero - 1))
  test.one = (sqrt(1 / h.one2zero) * test.one - (sqrt(2 * .l) - log(pi * .l) / sqrt(2 * .l))) / (1 / sqrt(2 * .l))
  for(j in 1:(2 * d)){
    test.one.jump[j] = (sqrt(1 / h.one2zero) * test.one.jump[j] - (sqrt(2 * .l) - log(pi * .l) / sqrt(2 * .l))) / (1 / sqrt(2 * .l))
  }
  
  
  # test decision
  alpha.asymp.one = test.one      >  quant.asymp # 1 if false alarm; 0 else
   beta.asymp.one = test.one.jump <= quant.asymp # 1 if jump not detected; 0 else
  
  
   # return results
  c(jump.time,
    test.one.time,
    test.one,
    test.one.jump,
    alpha.asymp.one,
    beta.asymp.one)
}

#############################################################################################

boot.global = function(sigma2, # variance process
                       q.hat.one, # estimated noise parameter under LOMN
                       q.hat.two, # estimated noise parameter under MMN
                       h.one, # number of observations per window under LOMN
                       h.two, # number of observations per window under MMN
                       q2c, # scaling constant needed for asym. inference under MMN
                       n, # sample size
                       datpoints.one, # data points for local windows
                       datpoints.two # data points for local windows
                       ){
  
  # sample the equilibrium price process
  X = c(filter(x = c(0, sqrt(sigma2) * sqrt(1/n) * zrnorm(n = n)), 
               filter = 1, 
               method = "recursive", 
               init = 1))
  
  # random sample of iid Gaussian for the noise
  z = zrnorm(n = n + 1)
  
  # price processes under H0 for LOMN and MMN    
  Y.one = X + abs(q.hat.one * z) / sqrt((1 - 2/pi))
  Y.two = X + q.hat.two * z
  
  # return results of one bootstrap iteration
  c(max(abs(diff(row_min(t(matrix(Y.one[datpoints.one], nrow = h.one)))))/sqrt(sigma2[round(colMeans(datpoints.one))][-1]), na.rm = T),
  max(abs(diff(colMeans(matrix(Y.two[datpoints.two], nrow = h.two))))/sqrt(2/3 * sigma2[round(colMeans(datpoints.two))][-1] * q2c^2 + 2 * q.hat.two^2), na.rm = T))
}

#############################################################################################

boot.global.round = function(sigma2, # variance process
                             q.hat.two, # estimated noise parameter under MMN 
                             h, # number of observations per window
                             q2c, # scaling constant needed for asym. inference under MMN
                             n, # sample size
                             S0, # initial value for efficient price process
                             datpoints.two # data points for local windows
                             ){
  
  # sample the equilibrium price process
  X = c(filter(x = c(0, sqrt(sigma2) * sqrt(1/n) * zrnorm(n = n)), 
               filter = 1, 
               method = "recursive", 
               init = log(S0)))
  
  
  z = zrnorm(n = n + 1)
  
  # price processes under H0 for MMN
  Y.two = X + q.hat.two * z
  
  # price processes under H0 for MMN
  w = which(z >= 0)
  
  n.ask = length(w)
  n.bid = length(z) - n.ask
  
  datpoints.ask = rep_row(seq(1, n.ask, by = h), h) + rep_col(0:(h - 1), ceiling(n.ask / h))
  datpoints.bid = rep_row(seq(1, n.bid, by = h), h) + rep_col(0:(h - 1), ceiling(n.bid / h))
  
  Y.ask = Y.two[ w]
  Y.bid = Y.two[-w]
  
  # return results of one bootstrap iteration
  c(max(abs(diff(row_min(t(matrix(Y.ask[datpoints.ask], nrow = h)))))/sqrt(sigma2[round(colMeans(matrix(w[datpoints.ask], nrow = h)))][-1]), na.rm = T),
    max(abs(diff(row_max(t(matrix(Y.bid[datpoints.bid], nrow = h)))))/sqrt(sigma2[round(colMeans(matrix(w[datpoints.bid], nrow = h)))][-1]), na.rm = T),
    max(abs(diff(colMeans(matrix(Y.two[datpoints.two], nrow = h))))/sqrt(2/3 * sigma2[round(colMeans(datpoints.two))][-1] * q2c^2 + 2 * q.hat.two^2), na.rm = T))
}

#############################################################################################

sim.global = function(n, 
                      q,
                      h,
                      q2c){
  
  # volatility model
  sigma2 = numeric(n); sigma2[1] = .8465
  
  BM = sqrt(1/n) * matrix(zrnorm(n = 2 * n), 
                          ncol = 2) %*% 
                   matrix(c(-0.8660254, -0.5, 0.8660254, -0.5), 
                          ncol = 2)
  
  for(i in 1:(n - 1)){
    sigma2[i + 1] = sigma2[i] + 0.0162 * (0.8465 - sigma2[i])/n + 0.117 * sqrt(sigma2[i]) * BM[i, 2]
  }
    
  # rescale squared volatility
  sigma2 = sigma2 * ((6 - sin(3/4 * pi * (1:n)/n))/5 * 0.01)^2
  
  
  # equilibrium log-price process under H0
  X = c(filter(x = c(0, sqrt(sigma2) * BM[, 1]), 
               filter = 1, 
               method = "recursive", 
               init = 1))
  
  
  # define jump time, jump size and equilibrium jump-log-price process under H1
  jump.time = sample(2850:20580, 1)
  jump.size = c(0.075, 0.10, 0.125, 0.15, 0.175, 0.20, 2.25) / 100
  
  d = length(jump.size)
  X.jump = matrix(0, 
                  nrow = length(X), 
                  ncol = 2 * d)
  jump.time = sample(2850:20580, 1)
  for(j in 1:d){
    X.jump[, j] = X
    X.jump[jump.time:(n + 1), j] = X.jump[jump.time:(n + 1), j] + jump.size[j]
  }
  for(j in 1:d){
    X.jump[, d + j] = X
    X.jump[jump.time:(n + 1), d + j] = X.jump[jump.time:(n + 1), d + j] - jump.size[j]
  }
  
  
  # define noise under MMN and LOMN
  veps = q * zrnorm(n = n + 1)
   eta = abs(veps) / sqrt((1 - 2/pi)) 
  
   
  # 4 processes: hypothesis (no jump) and alternative (with jump), one and two-sided noise
  Y.one = X + eta
  Y.two = X + veps
  Y.one.jump = X.jump + matrix(rep(eta, 2 * d), ncol = 2 * d)
  Y.two.jump = X.jump + matrix(rep(veps, 2 * d), ncol = 2 * d)
  
  
  # estimate sd of noise
  q.hat.one = sqrt(1/2 * mean(diff(Y.one.jump)^2))
  q.hat.two = sqrt(1/2 * mean(diff(Y.two.jump)^2))
  
  
  # map number of observations per local window
  h.one = h
  h.two = h
  
  
  # define data points used for the local windows to compute local minima
  datpoints.one = rep_row(seq(1, n, by = h.one), h.one) + rep_col(0:(h.one - 1), ceiling(n / h.one))
  
  # compute test statistics under H0 for LOMN
  test.one = abs(diff(row_min(t(matrix(Y.one[datpoints.one], nrow = h.one)))))/sqrt(sigma2[round(colMeans(datpoints.one))][-1])
  test.one = max(test.one, na.rm = T)  
  
  # compute test statistics under H1 for LOMN
  test.one.jump = numeric(2 * d)
  for(j in 1:(2 * d)){
    Xtest.one.jump =     abs(diff(row_min(t(matrix(Y.one.jump[, j][datpoints.one], nrow = h.one)))))/sqrt(sigma2[round(colMeans(datpoints.one))][-1])
    test.one.jump[j] = max(Xtest.one.jump, na.rm = T)  
  }
  
  
  # define data points used for the local windows to compute local averages
  datpoints.two = rep_row(seq(1, n, by = h.two), h.two) + rep_col(0:(h.two - 1), ceiling(n / h.two))
  
  # compute asymptotic variance of local statistic under MMN
  V.two         = sqrt(2/3 * sigma2[round(colMeans(datpoints.two))][-1] * q2c^2 + 2 * q.hat.two^2)
  
  # compute test statistics under H0 for MMN
  test.two      = abs(diff(colMeans(matrix(Y.two[datpoints.two], nrow = h.two)))) / V.two
  test.two      = max(test.two, na.rm = T)
  
  # compute test statistics under H1 for MMN
  test.two.jump = numeric(2 * d)
  for(j in 1:(2 * d)){
    Xtest.two.jump =     abs(diff(colMeans(matrix(Y.two.jump[, j][datpoints.two], nrow = h.two)))) / V.two
    test.two.jump[j] = max(Xtest.two.jump, na.rm = T)  
  }
  
  
  # test decision based on bootstrap
  draws <- 5000 # define number of bootstrap replications
  maxs = matrix(0, 
                nrow = draws, 
                ncol = 2) # define array to fetch results of bootstrap
  for(s in 1:draws){
    maxs[s, ] = boot.global(sigma2 = sigma2, 
                            q.hat.one = q.hat.one,
                            q.hat.two = q.hat.two,
                            h.one = h.one,
                            h.two = h.two,
                            q2c = q2c,
                            n = n,
                            datpoints.one = datpoints.one,
                            datpoints.two = datpoints.two)
  }
  
  
  # compute quantile of bootstrap-sample for LOMN
  quant.boot.one = quantile(maxs[, 1], 
                            p = 0.95)
  
  
  # test decision for LOMN
  alpha.boot.one = test.one > quant.boot.one
  beta.boot.one = test.one.jump <= quant.boot.one
  
  
  # compute quantile of bootstrap-sample for MMN
  quant.boot.two = quantile(maxs[, 2], 
                            p = 0.95)
  
  # test decision for MMN
  alpha.boot.two = test.two > quant.boot.two
  beta.boot.two = test.two.jump <= quant.boot.two

  
  # return results
  c(test.one,
    test.one.jump,
    alpha.boot.one,
    beta.boot.one,
    test.two,
    test.two.jump,
    alpha.boot.two,
    beta.boot.two)
}

#############################################################################################

sim.global.round = function(n, 
                            q,
                            h,
                            q2c,
                            S0,
                            tsize){
  
  
  # volatility model
  sigma2 = numeric(n); sigma2[1] = .8465
  
  BM = sqrt(1/n) * matrix(zrnorm(n = 2 * n), 
                          ncol = 2) %*% 
    matrix(c(-0.8660254, -0.5, 0.8660254, -0.5), 
           ncol = 2)
  
  for(i in 1:(n - 1)){
    sigma2[i + 1] = sigma2[i] + 0.0162 * (0.8465 - sigma2[i])/n + 0.117 * sqrt(sigma2[i]) * BM[i, 2]
  }
  
  
  # rescale squared volatility
  sigma2 = sigma2 * ((6 - sin(3/4 * pi * (1:n)/n))/5 * 0.01)^2
  
  
  # equilibrium log-price process under H0
  X = c(filter(x = c(0, sqrt(sigma2) * BM[, 1]), 
               filter = 1, 
               method = "recursive", 
               init = log(S0)))
  
  
  # define jump time, jump size and equilibrium jump-log-price process under H1
  jump.time = sample(2850:20580, 1)
  jump.size = c(0.075, 0.10, 0.125, 0.15, 0.175, 0.20, 2.25) / 100
  
  d = length(jump.size)
  X.jump = matrix(0, 
                  nrow = length(X), 
                  ncol = 2 * d)
  jump.time = sample(2850:20580, 1)
  for(j in 1:d){
    X.jump[, j] = X
    X.jump[jump.time:(n + 1), j] = X.jump[jump.time:(n + 1), j] + jump.size[j]
  }
  for(j in 1:d){
    X.jump[, d + j] = X
    X.jump[jump.time:(n + 1), d + j] = X.jump[jump.time:(n + 1), d + j] - jump.size[j]
  }
  
  
  # define noise under MMN
  veps = q * zrnorm(n = n + 1)
  
  
  # 2 processes: hypothesis (no jump) and alternative (with jump) for two-sided noise
  Y.two = cbind(1:(n + 1), log(round(exp(X + veps) / tsize) * tsize))
  Y.two.jump = cbind(1:(n + 1), log(round(exp(X.jump + matrix(rep(veps, 2 * d), ncol = 2 * d)) / tsize) * tsize))
  
  
  # 4 processes: hypothesis (no jump) and alternative (with jump) for one-sided noise
  w = which(Y.two[, 2] >= X)
  Y.ask = Y.two[ w,]
  Y.bid = Y.two[-w,]
  
  Y.ask.jump = Y.bid.jump = list(); length(Y.ask.jump) = length(Y.bid.jump) = 2 * d
  for(j in 1:(2 * d)){
    w = which(Y.two.jump[, j + 1] >= X.jump[, j])
    Y.ask.jump[[j]] = Y.two.jump[ w, c(1, j + 1)]
    Y.bid.jump[[j]] = Y.two.jump[-w, c(1, j + 1)]  
  }
  
  
  # estimate sd of noise
  q.hat.two = mean(apply(Y.two.jump[, -1], 2, FUN = function(r){sqrt(1/2 * mean(diff(r)^2))}))
  
  
  # define data points used for the local windows to compute local minima
  n.ask = c(NROW(Y.ask), sapply(Y.ask.jump, NROW))
  datpoints.ask = list(); length(datpoints.ask) = 2 * d + 1
  for(j in 1:(2 * d + 1)){
    datpoints.ask[[j]] = rep_row(seq(1, n.ask[j], by = h), h) + rep_col(0:(h - 1), ceiling(n.ask[j] / h))   
  }
  
  # compute test statistics under H0 for LOMN
  test.ask = abs(diff(row_min(t(matrix(Y.ask[, 2][datpoints.ask[[1]]], nrow = h)))))/sqrt(sigma2[round(colMeans(matrix(Y.ask[, 1][datpoints.ask[[1]]], nrow = h)))][-1])
  test.ask = max(test.ask, na.rm = T)  
  
  
  # compute test statistics under H1 for LOMN
  test.ask.jump = numeric(2 * d)
  for(j in 1:(2 * d)){
    Xtest.ask.jump   = abs(diff(row_min(t(matrix(Y.ask.jump[[j]][, 2][datpoints.ask[[j + 1]]], nrow = h)))))/sqrt(sigma2[round(colMeans(matrix(Y.ask.jump[[j]][, 1][datpoints.ask[[j + 1]]], nrow = h)))][-1])
    test.ask.jump[j] = max(Xtest.ask.jump, na.rm = T)  
  }
  
  
  # define data points used for the local windows to compute local maxima
  n.bid = c(NROW(Y.bid), sapply(Y.bid.jump, NROW))
  datpoints.bid = list(); length(datpoints.bid) = 2 * d + 1
  for(j in 1:(2 * d + 1)){
    datpoints.bid[[j]] = rep_row(seq(1, n.bid[j], by = h), h) + rep_col(0:(h - 1), ceiling(n.bid[j] / h))   
  }
  
  
  # compute test statistics under H0 for LOMN  
  test.bid      =     abs(diff(row_max(t(matrix(Y.bid[, 2][datpoints.bid[[1]]], nrow = h)))))/sqrt(sigma2[round(colMeans(matrix(Y.bid[, 1][datpoints.bid[[1]]], nrow = h)))][-1])
  test.bid      = max(test.bid, na.rm = T)  
  
  
  # compute test statistics under H1 for LOMN 
  test.bid.jump = numeric(2 * d)
  for(j in 1:(2 * d)){
    Xtest.bid.jump   =     abs(diff(row_max(t(matrix(Y.bid.jump[[j]][, 2][datpoints.bid[[j + 1]]], nrow = h)))))/sqrt(sigma2[round(colMeans(matrix(Y.bid.jump[[j]][, 1][datpoints.bid[[j + 1]]], nrow = h)))][-1])
    test.bid.jump[j] = max(Xtest.bid.jump, na.rm = T)  
  }
  
  
  # define data points used for the local windows to compute local averages
  datpoints.two = rep_row(seq(1, n, by = h), h) + rep_col(0:(h - 1), ceiling(n / h))
  
  
  # compute asymptotic variance of local statistic under MMN
  V.two = sqrt(2/3 * sigma2[round(colMeans(datpoints.two))][-1] * q2c^2 + 2 * q.hat.two^2)
  
  
  # compute test statistics under H0 for MMN
  test.two      = abs(diff(colMeans(matrix(Y.two[,2][datpoints.two], nrow = h)))) / V.two
  test.two      = max(test.two, na.rm = T)
  
  
  # compute test statistics under H1 for MMN
  test.two.jump = test.two.time = numeric(2 * d)
  for(j in 1:(2 * d)){
    Xtest.two.jump =     abs(diff(colMeans(matrix(Y.two.jump[, j + 1][datpoints.two], nrow = h)))) / V.two
    test.two.jump[j] = max(Xtest.two.jump, na.rm = T)  
  }
  
  
  # test decision based on bootstrap
  draws <- 5000 # define number of bootstrap replications
  maxs = matrix(0, 
                nrow = draws, 
                ncol = 3) # define array to fetch results of bootstrap
  for(s in 1:draws){
    maxs[s, ] = boot.global.round(sigma2 = sigma2, 
                                  q.hat.two = q.hat.two,
                                  h = h,
                                  q2c = q2c,
                                  n = n,
                                  S0 = S0,
                                  datpoints.two = datpoints.two)
  }
  
  
  # compute quantile of bootstrap-sample for LOMN (ask-quotes)
  quant.boot.ask = quantile(maxs[, 1], 
                            p = 0.95)
  
  
  # test decision for LOMN (ask-quotes)
  alpha.boot.ask = test.ask > quant.boot.ask
  beta.boot.ask = test.ask.jump <= quant.boot.ask
  
  
  # compute quantile of bootstrap-sample for LOMN (bid-quotes)
  quant.boot.bid = quantile(maxs[, 2], 
                            p = 0.95)
  
  
  # test decision for LOMN (bid-quotes)
  alpha.boot.bid = test.bid > quant.boot.bid
  beta.boot.bid = test.bid.jump <= quant.boot.bid
  
  
  # compute quantile of bootstrap-sample for MMN
  quant.boot.two = quantile(maxs[, 3], 
                            p = 0.95)
  
  
  # test decision for MMN
  alpha.boot.two = test.two > quant.boot.two
  beta.boot.two = test.two.jump <= quant.boot.two
  
  
  #return results
  c(q.hat.two,
    test.ask,
    test.ask.jump,
    alpha.boot.ask,
    beta.boot.ask,
    test.bid,
    test.bid.jump,
    alpha.boot.bid,
    beta.boot.bid,
    test.two,
    test.two.jump,
    alpha.boot.two,
    beta.boot.two)
}

#############################################################################################

boot.local = function(sq.vola, # spot variance at jump time
                      q.hat.one, # estimated noise level under LOMN
                      q.hat.two, # estimated noise level under MMN
                      h, # number of observations per local window
                      n # sample size
                      ){
  
  # sample equilibrium price process
  X = c(filter(x = c(0, sqrt(sq.vola / n) * zrnorm(n = n)), 
               filter = 1, 
               method = "recursive", 
               init = 1))
  
  
  # sample Gaussian random variables
  veps = zrnorm(n = n + 1)
  
  
  # sample price process contaminated by noise for the LOMN and the MMN
  Y.one = X + q.hat.one * abs(veps) / sqrt((1 - 2/pi))
  Y.two = X + q.hat.two * veps
  
  
  # return results of one bootstrap iteration
  cbind(
    abs(diff(row_min(t(matrix(Y.one[rep_row(seq(1, n, by = h), h) + rep_col(0:(h - 1), ceiling(n/h))], nrow = h))))),
    abs(diff(colMeans(matrix(Y.two[rep_row(seq(1, n, by = h), h) + rep_col(0:(h - 1), ceiling(n/h))], nrow = h)))))
}

#############################################################################################

sim.local = function(n, #sample size
                     q, #noise level
                     h  #number of observations per time interval
                     ){
  
  # volatility model
  sigma2 = numeric(n); sigma2[1] = .8465
  
  BM = sqrt(1/n) * matrix(zrnorm(n = 2 * n), 
                          ncol = 2) %*% 
                   matrix(c(-0.8660254, -0.5, 0.8660254, -0.5), 
                          ncol = 2)
  
  for(i in 1:(n - 1)){
    sigma2[i + 1] = sigma2[i] + 0.0162 * (0.8465 - sigma2[i])/n + 0.117 * sqrt(sigma2[i]) * BM[i, 2]
  }
  
  # re-scale squared volatility
  sigma2 = sigma2 * ((6 - sin(3/4 * pi * (1:n)/n))/5 * 0.01)^2
  
  
  # equilibrium log-price process
  X = c(filter(x = c(0, sqrt(sigma2) * BM[, 1]), 
               filter = 1, 
               method = "recursive", 
               init = 1))
  
  
  # define (random) jump time and the jump sizes
  jump.time = sample(2850:20580, 1)
  jump.size = c(0.05, 0.075, 0.10, 0.125, 0.15, 0.20) / 100
  
  # equilibrium jump-log-price process under H1
  d = length(jump.size)
  X.jump = matrix(0, 
                  nrow = length(X), 
                  ncol = 2 * d)
  
  for(j in 1:d){
    X.jump[, j] = X
    X.jump[jump.time:(n + 1), j] = X.jump[jump.time:(n + 1), j] + jump.size[j]
  }
  for(j in 1:d){
    X.jump[, d + j] = X
    X.jump[jump.time:(n + 1), d + j] = X.jump[jump.time:(n + 1), d + j] - jump.size[j]
  }
  
  
  # sample the noise under MMN and LOMN
  veps = q * zrnorm(n = n + 1)
   eta = abs(veps) / sqrt((1 - 2/pi)) 
  
   
  # 4 processes: hypothesis (no jump) and alternative (with jump), one and two-sided noise
  Y.one = X + eta
  Y.two = X + veps
  Y.one.jump = X.jump + matrix(rep(eta, 2 * d), ncol = 2 * d)
  Y.two.jump = X.jump + matrix(rep(veps, 2 * d), ncol = 2 * d)
  
  
  # estimate noise level
  q.hat.one = sqrt(1/2 * mean(diff(Y.one.jump)^2))
  q.hat.two = sqrt(1/2 * mean(diff(Y.two.jump)^2))
  
  
  # define test times with jump being left and jump being right of test time
  rand.one = sample(1:(h - 1), 1)
  jump.time.left  = jump.time + rand.one
  jump.time.right = jump.time - rand.one
  
  
  # test statistics under H0 for LOMN
  test.one = abs(min(Y.one[jump.time:(jump.time + h - 1)]) - min(Y.one[(jump.time - h):(jump.time - 1)]))
  test.left.one = abs(min(Y.one[jump.time.left:(jump.time.left + h - 1)]) - min(Y.one[(jump.time.left - h):(jump.time.left - 1)]))
  test.right.one = abs(min(Y.one[jump.time.right:(jump.time.right + h - 1)]) - min(Y.one[(jump.time.right - h):(jump.time.right - 1)]))
  
  
  # test statistics under H1 for LOMN
  test.one.jump = test.left.one.jump = test.right.one.jump = numeric(2 * d)
  for(j in 1:(2 * d)){
    test.one.jump[j] = abs(min(Y.one.jump[jump.time:(jump.time + h - 1), j]) - min(Y.one.jump[(jump.time - h):(jump.time - 1), j]))
    test.left.one.jump[j] = abs(min(Y.one.jump[jump.time.left:(jump.time.left + h - 1), j]) - min(Y.one.jump[(jump.time.left - h):(jump.time.left - 1), j]))
    test.right.one.jump[j] = abs(min(Y.one.jump[jump.time.right:(jump.time.right + h - 1), j]) - min(Y.one.jump[(jump.time.right - h):(jump.time.right - 1), j]))
  }
  
  
  # test statistics under H0 for MMN
  test.two = abs(mean(Y.two[jump.time:(jump.time + h - 1)]) - mean(Y.two[(jump.time - h):(jump.time - 1)]))
  test.left.two = abs(mean(Y.two[jump.time.left:(jump.time.left + h - 1)]) - mean(Y.two[(jump.time.left - h):(jump.time.left - 1)]))
  test.right.two = abs(mean(Y.two[jump.time.right:(jump.time.right + h - 1)]) - mean(Y.two[(jump.time.right - h):(jump.time.right - 1)]))
  
  
  # test statistics under H1 for MMN
  test.two.jump = test.left.two.jump = test.right.two.jump = numeric(2 * d)
  for(j in 1:(2 * d)){
    test.two.jump[j] = abs(mean(Y.two.jump[jump.time:(jump.time + h - 1), j]) - mean(Y.two.jump[(jump.time - h):(jump.time - 1), j]))
    test.left.two.jump[j] = abs(mean(Y.two.jump[jump.time.left:(jump.time.left + h - 1), j]) - mean(Y.two.jump[(jump.time.left - h):(jump.time.left - 1), j]))
    test.right.two.jump[j] = abs(mean(Y.two.jump[jump.time.right:(jump.time.right + h - 1), j]) - mean(Y.two.jump[(jump.time.right - h):(jump.time.right - 1), j]))
  }
  
  
  # test decision based on bootstrap
  boots = boot.local(sq.vola = sigma2[jump.time], 
                     q.hat.one = q.hat.one,
                     q.hat.two = q.hat.two,
                     h = h,
                     n = n)
  
  
  # quantile of bootstrap sample for LOMN
  q.boot.one = quantile(boots[, 1], 
                        p = 0.95,
                        na.rm = TRUE)
  
  
  # test decisions for LOMN
  alpha.boot.one = test.one > q.boot.one 
  beta.boot.one = test.one.jump <= q.boot.one
  
  alpha.boot.left.one = test.left.one > q.boot.one
  beta.boot.left.one = test.left.one.jump <= q.boot.one
  
  alpha.boot.right.one = test.right.one > q.boot.one
  beta.boot.right.one = test.right.one.jump <= q.boot.one  
  
  
  # quantile of bootstrap sample for MMN
  q.boot.two = quantile(boots[, 2], 
                        p = 0.95,
                        na.rm = TRUE)
  
  
  # test decisions for MMN
  alpha.boot.two = test.two > q.boot.two
  beta.boot.two = test.two.jump <= q.boot.two
  
  alpha.boot.left.two = test.left.two > q.boot.two   
  beta.boot.left.two = test.left.two.jump <= q.boot.two
  
  alpha.boot.right.two = test.right.two > q.boot.two
  beta.boot.right.two = test.right.two.jump <= q.boot.two
  
  
  # return results  
  c(alpha.boot.one,
    beta.boot.one,
    alpha.boot.two,
    beta.boot.two,
    alpha.boot.left.one,
    beta.boot.left.one,
    alpha.boot.left.two,
    beta.boot.left.two,
    alpha.boot.right.one,
    beta.boot.right.one,
    alpha.boot.right.two,
    beta.boot.right.two)
}