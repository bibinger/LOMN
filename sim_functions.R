#############################################################################################
# Function        Description of function (arguments explained in function)
#############################################################################################
# sq.vola.global  function computes the spot variance in the LOMN model 
# sim.global.one  wraps the steps for the simulation in the LOMN model (asymptotic critical values)
# boot.global     function conducts the bootstrap for critical values of the global test
# sim.global      wraps the steps for the simulation of the global test LOMN vs MMN (bootstrap for critical values)
# boot.local      function conducts the bootstrap for critical values of the local test
# sim.local       wraps the steps for the simulation of the local test LOMN vs MMN (bootstrap for critical values)
#############################################################################################

sq.vola.global = function(Y, # noisy observations generated by LOMN-model
                          h, # number of observations per time interval (local minima)
                          K, # width of the smoothing window
                          n  # number of observations
                          ){
  
  # define data points to compute local minima
  sgrid = seq(h + 1, n, by = h); g = length(sgrid)
  datpoints = c(rep_row(sgrid - (K + 1) * h, 2 * K + 2) + rep_col(seq(0, (2 * K + 1) * h, by = h), length(sgrid)))
  datpoints[datpoints < h | datpoints > n - h] = NA
  
  
  # compute spot variance
  sq.vola = n/(2 * h) * pi/(pi - 2) * colMeans(matrix(c(NA, diff(row_min(t(matrix(Y[rep_row(datpoints, h) + rep_col(0:(h - 1), length(datpoints))], nrow = h))))^2), nrow = 2 * (K + 1))[-1, ], na.rm = T)
  
  
  # return results
  c(rep(sq.vola[1], ceiling(sum(sgrid[1:2])/2 - 1)),
    sapply(sq.vola[-c(1, g)], rep, h),
    rep(sq.vola[g], floor(n - sum(sgrid[g:(g - 1)])/2 + 1)))
}

#############################################################################################

sim.global.one = function(n, 
                          q){
  
  # volatility model
  sigma2 = numeric(n); sigma2[1] = .8465
  
      BM = sqrt(1/n) * matrix(zrnorm(n = 2 * n), 
                              ncol = 2) %*% 
                       matrix(c(-0.8660254, -0.5, 0.8660254, -0.5), 
                              ncol = 2)
  
  for(i in 1:(n - 1)){
    sigma2[i + 1] = sigma2[i] + 0.0162 * (0.8465 - sigma2[i])/n + 0.117 * sqrt(sigma2[i]) * BM[i, 2]
  }
  
  # rescale squared volatility
  sigma2 = sigma2 * ((6 - sin(3/4 * pi * (1:n)/n))/5 * 0.01)^2
  
  
  # equilibrium log-price process under H0
  X = c(filter(x = c(0, sqrt(sigma2) * BM[, 1]), 
               filter = 1, 
               method = "recursive", 
               init = 1))
  
  
  # define jump size, jump time and equilibrium jump-log-price process under H1
  jump.size = c(0.05, 0.075, 0.10, 0.15, 0.20, 0.25) / 100
  d = length(jump.size)
  X.jump = matrix(0, 
                  nrow = length(X), 
                  ncol = 2 * d)
  jump.time = sample(2850:20580, 1)
  for(j in 1:d){
    X.jump[, j] = X
    X.jump[jump.time:(n + 1), j] = X.jump[jump.time:(n + 1), j] + jump.size[j]
  }
  for(j in 1:d){
    X.jump[, d + j] = X
    X.jump[jump.time:(n + 1), d + j] = X.jump[jump.time:(n + 1), d + j] - jump.size[j]
  }  
  
  
  # one-sided noise  
  eta = rexp(n = n + 1, 
             rate = 1 / q)
  
  
  # two processes: hypothesis (no jump) and alternative (with jump)
  Y.one = X + eta
  Y.one.jump = X.jump + matrix(rep(eta, 2 * d), ncol = 2 * d)
  
  
  # length of local windows
  h.one2zero = 1.2 * n^(-2/3) # as defined in theory
  h.one = ceiling(n * h.one2zero) # transformed to number of observations per window
  
  
  # spot variance estimation
  Sq.vola.one = sq.vola.global(Y = Y.one,
                               h = 30, 
                               K = 100,
                               n = n) * 0.954
  
  
  # define data points used for the local windows to compute local minima
  datpoints.one = rep_row(seq(1, n, by = h.one), h.one) + rep_col(0:(h.one - 1), ceiling(n/h.one))
  
  
  # test statistics under H0
  test.one = abs(diff(row_min(t(matrix(Y.one[datpoints.one], nrow = h.one)))))/sqrt(Sq.vola.one[round(colMeans(datpoints.one))][-1])
  test.one = max(test.one, na.rm = T)
  
  
  # test statistics under H1
  test.one.jump = test.one.time = numeric(2 * d)
  for(j in 1:(2 * d)){
    Xtest.one.jump   = abs(diff(row_min(t(matrix(Y.one.jump[, j][datpoints.one], nrow = h.one)))))/sqrt(Sq.vola.one[round(colMeans(datpoints.one))][-1])
    test.one.time[j] = which.max(Xtest.one.jump)
    test.one.jump[j] = max(Xtest.one.jump, na.rm = T)  
  }
  
  # quantile of Gumbel-distribution
  quant.asymp = -log(-log(0.95))
  
  
  # rescale test-statistics               
        .l = log(2 * (1 / h.one2zero - 1))
  test.one = (sqrt(1 / h.one2zero) * test.one - (sqrt(2 * .l) - log(pi * .l) / sqrt(2 * .l))) / (1 / sqrt(2 * .l))
  for(j in 1:(2 * d)){
    test.one.jump[j] = (sqrt(1 / h.one2zero) * test.one.jump[j] - (sqrt(2 * .l) - log(pi * .l) / sqrt(2 * .l))) / (1 / sqrt(2 * .l))
  }
  
  
  # test decision
  alpha.asymp.one = test.one      >  quant.asymp # 1 if false alarm; 0 else
   beta.asymp.one = test.one.jump <= quant.asymp # 1 if jump not detected; 0 else
  
  
   # return results
  c(jump.time,
    test.one.time * h.one,
    test.one,
    test.one.jump,
    alpha.asymp.one,
    beta.asymp.one)
}

#############################################################################################

boot.global = function(sigma2, # variance process
                       q.hat.one, # estimated noise parameter under LOMN
                       q.hat.two, # estimated noise parameter under MMN
                       h.one, # number of observations per window under LOMN
                       h.two, # number of observations per window under MMN
                       q2c, # scaling constant needed for asym. inference under MMN
                       n, # sample size
                       datpoints.one, # data points for local windows
                       datpoints.two # data points for local windows
                       ){
  
  # sample the equilibrium price process
  X = c(filter(x = c(0, sqrt(sigma2) * sqrt(1/n) * zrnorm(n = n)), 
               filter = 1, 
               method = "recursive", 
               init = 1))
  
  # random sample of iid Gaussian for the noise
  z = zrnorm(n = n + 1)
  
  # price processes under H0 for LOMN and MMN    
  Y.one = X + abs(q.hat.one * z) / sqrt((1 - 2/pi))
  Y.two = X + q.hat.two * z
  
  # return results of one bootstrap iteration
  c(max(abs(diff(row_min(t(matrix(Y.one[datpoints.one], nrow = h.one)))))/sqrt(sigma2[round(colMeans(datpoints.one))][-1]), na.rm = T),
  max(abs(diff(colMeans(matrix(Y.two[datpoints.two], nrow = h.two))))/sqrt(2/3 * sigma2[round(colMeans(datpoints.two))][-1] * q2c^2 + 2 * q.hat.two^2), na.rm = T))
}

#############################################################################################

sim.global = function(n, 
                      q,
                      h,
                      q2c){
  
  # volatility model
  sigma2 = numeric(n); sigma2[1] = .8465
  
  BM = sqrt(1/n) * matrix(zrnorm(n = 2 * n), 
                          ncol = 2) %*% 
                   matrix(c(-0.8660254, -0.5, 0.8660254, -0.5), 
                          ncol = 2)
  
  for(i in 1:(n - 1)){
    sigma2[i + 1] = sigma2[i] + 0.0162 * (0.8465 - sigma2[i])/n + 0.117 * sqrt(sigma2[i]) * BM[i, 2]
  }
    
  # rescale squared volatility
  sigma2 = sigma2 * ((6 - sin(3/4 * pi * (1:n)/n))/5 * 0.01)^2
  
  
  # equilibrium log-price process under H0
  X = c(filter(x = c(0, sqrt(sigma2) * BM[, 1]), 
               filter = 1, 
               method = "recursive", 
               init = 1))
  
  
  # define jump size, jump time and equilibrium jump-log-price process under H1
  jump.time = sample(2850:20580, 1)
  jump.size = c(0.05, 0.075, 0.10, 0.125, 0.15, 0.175, 0.20, 2.25, 2.5, 2.75, 3) / 100
  
  d = length(jump.size)
  X.jump = matrix(0, 
                  nrow = length(X), 
                  ncol = 2 * d)
  jump.time = sample(2850:20580, 1)
  for(j in 1:d){
    X.jump[, j] = X
    X.jump[jump.time:(n + 1), j] = X.jump[jump.time:(n + 1), j] + jump.size[j]
  }
  for(j in 1:d){
    X.jump[, d + j] = X
    X.jump[jump.time:(n + 1), d + j] = X.jump[jump.time:(n + 1), d + j] - jump.size[j]
  }
  
  
  # define noise under MMN and LOMN
  veps = q * zrnorm(n = n + 1)
   eta = abs(veps) / sqrt((1 - 2/pi)) 
  
   
  # 4 processes: hypothesis (no jump) and alternative (with jump), one and two-sided noise
  Y.one = X + eta
  Y.two = X + veps
  Y.one.jump = X.jump + matrix(rep(eta, 2 * d), ncol = 2 * d)
  Y.two.jump = X.jump + matrix(rep(veps, 2 * d), ncol = 2 * d)
  
  
  # estimate sd of noise
  q.hat.one = sqrt(1/2 * mean(diff(Y.one.jump)^2))
  q.hat.two = sqrt(1/2 * mean(diff(Y.two.jump)^2))
  
  
  # map number of observations per local window
  h.one = h
  h.two = h
  
  
  # define data points used for the local windows to compute local minima
  datpoints.one = rep_row(seq(1, n, by = h.one), h.one) + rep_col(0:(h.one - 1), ceiling(n / h.one))
  
  # compute test statistics under H0 for LOMN
  test.one = abs(diff(row_min(t(matrix(Y.one[datpoints.one], nrow = h.one)))))/sqrt(sigma2[round(colMeans(datpoints.one))][-1])
  test.one = max(test.one, na.rm = T)  
  
  # compute test statistics under H1 for LOMN
  test.one.jump = test.one.time = numeric(2 * d)
  for(j in 1:(2 * d)){
    Xtest.one.jump =     abs(diff(row_min(t(matrix(Y.one.jump[, j][datpoints.one], nrow = h.one)))))/sqrt(sigma2[round(colMeans(datpoints.one))][-1])
    test.one.time[j] = which.max(Xtest.one.jump)
    test.one.jump[j] = max(Xtest.one.jump, na.rm = T)  
  }
  
  
  # define data points used for the local windows to compute local averages
  datpoints.two = rep_row(seq(1, n, by = h.two), h.two) + rep_col(0:(h.two - 1), ceiling(n / h.two))
  
  # compute asymptotic variance of local statistic under MMN
  V.two         = sqrt(2/3 * sigma2[round(colMeans(datpoints.two))][-1] * q2c^2 + 2 * q.hat.two^2)
  
  # compute test statistics under H0 for MMN
  test.two      = abs(diff(colMeans(matrix(Y.two[datpoints.two], nrow = h.two)))) / V.two
  test.two      = max(test.two, na.rm = T)
  
  # compute test statistics under H1 for MMN
  test.two.jump = test.two.time = numeric(2 * d)
  for(j in 1:(2 * d)){
    Xtest.two.jump =     abs(diff(colMeans(matrix(Y.two.jump[, j][datpoints.two], nrow = h.two)))) / V.two
    test.two.time[j] = which.max(Xtest.two.jump)
    test.two.jump[j] = max(Xtest.two.jump, na.rm = T)  
  }
  
  
  # test decision based on bootstrap
  draws <- 5000 # define number of bootstrap replications
  maxs = matrix(0, nrow = draws, ncol = 2) # define array to fetch results of bootstrap
  for(s in 1:draws){
    maxs[s, ] = boot.global(sigma2 = sigma2, 
                            q.hat.one = q.hat.one,
                            q.hat.two = q.hat.two,
                            h.one = h.one,
                            h.two = h.two,
                            q2c = q2c,
                            n = n,
                            datpoints.one = datpoints.one,
                            datpoints.two = datpoints.two)
  }
  
  
  # compute quantile of bootstrap-sample for LOMN
  quant.boot.one = quantile(maxs[, 1], 
                            p = 0.95)
  
  
  # test decision for LOMN
  alpha.boot.one = test.one > quant.boot.one
  beta.boot.one = test.one.jump <= quant.boot.one
  
  
  # compute quantile of bootstrap-sample for MMN
  quant.boot.two = quantile(maxs[, 2], 
                            p = 0.95)
  
  # test decision for MMN
  alpha.boot.two = test.two > quant.boot.two
  beta.boot.two = test.two.jump <= quant.boot.two

  
  # return results
  c(jump.time,
    test.one.time * h.one,
    test.one,
    test.one.jump,
    alpha.boot.one,
    beta.boot.one,
    test.two.time * h.two,
    test.two,
    test.two.jump,
    alpha.boot.two,
    beta.boot.two)
}

#############################################################################################

boot.local = function(sq.vola, # spot variance at jump time
                      q.hat.one, # estimated noise level under LOMN
                      q.hat.two, # estimated noise level under MMN
                      h, # number of observations per local window
                      n # sample size
                      ){
  
  # sample equilibrium price process
  X = c(filter(x = c(0, sqrt(sq.vola / n) * zrnorm(n = n)), 
               filter = 1, 
               method = "recursive", 
               init = 1))
  
  
  # sample Gaussian random variables
  veps = zrnorm(n = n + 1)
  
  
  # sample price process contaminated by noise for the LOMN and the MMN
  Y.one = X + q.hat.one * abs(veps) / sqrt((1 - 2/pi))
  Y.two = X + q.hat.two * veps
  
  
  # return results of one bootstrap iteration
  cbind(
    abs(diff(row_min(t(matrix(Y.one[rep_row(seq(1, n, by = h), h) + rep_col(0:(h - 1), ceiling(n/h))], nrow = h))))),
    abs(diff(colMeans(matrix(Y.two[rep_row(seq(1, n, by = h), h) + rep_col(0:(h - 1), ceiling(n/h))], nrow = h)))))
}

#############################################################################################

sim.local = function(n, #sample size
                     q, #noise level
                     h  #number of observations per time interval
                     ){
  
  # volatility model
  sigma2 = numeric(n); sigma2[1] = .8465
  
  BM = sqrt(1/n) * matrix(zrnorm(n = 2 * n), 
                          ncol = 2) %*% 
                   matrix(c(-0.8660254, -0.5, 0.8660254, -0.5), 
                          ncol = 2)
  
  for(i in 1:(n - 1)){
    sigma2[i + 1] = sigma2[i] + 0.0162 * (0.8465 - sigma2[i])/n + 0.117 * sqrt(sigma2[i]) * BM[i, 2]
  }
  
  # re-scale squared volatility
  sigma2 = sigma2 * ((6 - sin(3/4 * pi * (1:n)/n))/5 * 0.01)^2
  
  
  # equilibrium log-price process
  X = c(filter(x = c(0, sqrt(sigma2) * BM[, 1]), 
               filter = 1, 
               method = "recursive", 
               init = 1))
  
  
  # define (random) jump time and the jump sizes
  jump.time = sample(2850:20580, 1)
  jump.size = c(0.05, 0.075, 0.10, 0.125, 0.15, 0.20) / 100
  
  # equilibrium jump-log-price process under H1
  d = length(jump.size)
  X.jump = matrix(0, 
                  nrow = length(X), 
                  ncol = 2 * d)
  
  for(j in 1:d){
    X.jump[, j] = X
    X.jump[jump.time:(n + 1), j] = X.jump[jump.time:(n + 1), j] + jump.size[j]
  }
  for(j in 1:d){
    X.jump[, d + j] = X
    X.jump[jump.time:(n + 1), d + j] = X.jump[jump.time:(n + 1), d + j] - jump.size[j]
  }
  
  
  # sample the noise under MMN and LOMN
  veps = q * zrnorm(n = n + 1)
   eta = abs(veps) / sqrt((1 - 2/pi)) 
  
   
  # 4 processes: hypothesis (no jump) and alternative (with jump), one and two-sided noise
  Y.one = X + eta
  Y.two = X + veps
  Y.one.jump = X.jump + matrix(rep(eta, 2 * d), ncol = 2 * d)
  Y.two.jump = X.jump + matrix(rep(veps, 2 * d), ncol = 2 * d)
  
  
  # estimate noise level
  q.hat.one = sqrt(1/2 * mean(diff(Y.one.jump)^2))
  q.hat.two = sqrt(1/2 * mean(diff(Y.two.jump)^2))
  
  
  # define test times with jump being left and jump being right of test time
  rand.one = sample(1:(h - 1), 1)
  jump.time.left  = jump.time + rand.one
  jump.time.right = jump.time - rand.one
  
  
  # test statistics under H0 for LOMN
  test.one = abs(min(Y.one[jump.time:(jump.time + h - 1)]) - min(Y.one[(jump.time - h):(jump.time - 1)]))
  test.left.one = abs(min(Y.one[jump.time.left:(jump.time.left + h - 1)]) - min(Y.one[(jump.time.left - h):(jump.time.left - 1)]))
  test.right.one = abs(min(Y.one[jump.time.right:(jump.time.right + h - 1)]) - min(Y.one[(jump.time.right - h):(jump.time.right - 1)]))
  
  
  # test statistics under H1 for LOMN
  test.one.jump = test.left.one.jump = test.right.one.jump = numeric(2 * d)
  for(j in 1:(2 * d)){
    test.one.jump[j] = abs(min(Y.one.jump[jump.time:(jump.time + h - 1), j]) - min(Y.one.jump[(jump.time - h):(jump.time - 1), j]))
    test.left.one.jump[j] = abs(min(Y.one.jump[jump.time.left:(jump.time.left + h - 1), j]) - min(Y.one.jump[(jump.time.left - h):(jump.time.left - 1), j]))
    test.right.one.jump[j] = abs(min(Y.one.jump[jump.time.right:(jump.time.right + h - 1), j]) - min(Y.one.jump[(jump.time.right - h):(jump.time.right - 1), j]))
  }
  
  
  # test statistics under H0 for MMN
  test.two = abs(mean(Y.two[jump.time:(jump.time + h - 1)]) - mean(Y.two[(jump.time - h):(jump.time - 1)]))
  test.left.two = abs(mean(Y.two[jump.time.left:(jump.time.left + h - 1)]) - mean(Y.two[(jump.time.left - h):(jump.time.left - 1)]))
  test.right.two = abs(mean(Y.two[jump.time.right:(jump.time.right + h - 1)]) - mean(Y.two[(jump.time.right - h):(jump.time.right - 1)]))
  
  
  # test statistics under H1 for MMN
  test.two.jump = test.left.two.jump = test.right.two.jump = numeric(2 * d)
  for(j in 1:(2 * d)){
    test.two.jump[j] = abs(mean(Y.two.jump[jump.time:(jump.time + h - 1), j]) - mean(Y.two.jump[(jump.time - h):(jump.time - 1), j]))
    test.left.two.jump[j] = abs(mean(Y.two.jump[jump.time.left:(jump.time.left + h - 1), j]) - mean(Y.two.jump[(jump.time.left - h):(jump.time.left - 1), j]))
    test.right.two.jump[j] = abs(mean(Y.two.jump[jump.time.right:(jump.time.right + h - 1), j]) - mean(Y.two.jump[(jump.time.right - h):(jump.time.right - 1), j]))
  }
  
  
  # test decision based on bootstrap
  boots = boot.local(sq.vola = sigma2[jump.time], 
                     q.hat.one = q.hat.one,
                     q.hat.two = q.hat.two,
                     h = h,
                     n = n)
  
  
  # quantile of bootstrap sample for LOMN
  q.boot.one = quantile(boots[, 1], 
                        p = 0.95,
                        na.rm = TRUE)
  
  
  # test decisions for LOMN
  alpha.boot.one = test.one > q.boot.one 
  beta.boot.one = test.one.jump <= q.boot.one
  
  alpha.boot.left.one = test.left.one > q.boot.one
  beta.boot.left.one = test.left.one.jump <= q.boot.one
  
  alpha.boot.right.one = test.right.one > q.boot.one
  beta.boot.right.one = test.right.one.jump <= q.boot.one  
  
  
  # quantile of bootstrap sample for MMN
  q.boot.two = quantile(boots[, 2], 
                        p = 0.95,
                        na.rm = TRUE)
  
  
  # test decisions for MMN
  alpha.boot.two = test.two > q.boot.two
  beta.boot.two = test.two.jump <= q.boot.two
  
  alpha.boot.left.two = test.left.two > q.boot.two   
  beta.boot.left.two = test.left.two.jump <= q.boot.two
  
  alpha.boot.right.two = test.right.two > q.boot.two
  beta.boot.right.two = test.right.two.jump <= q.boot.two
  
  
  # return results  
  c(alpha.boot.one,
    beta.boot.one,
    alpha.boot.two,
    beta.boot.two,
    alpha.boot.left.one,
    beta.boot.left.one,
    alpha.boot.left.two,
    beta.boot.left.two,
    alpha.boot.right.one,
    beta.boot.right.one,
    alpha.boot.right.two,
    beta.boot.right.two)
}